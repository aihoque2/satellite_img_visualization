{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Project.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"scrolled":true,"id":"wkBlQE5GLrPx","colab_type":"code","outputId":"4edad71b-7acb-4fb3-d24a-c4e40fa62cce","executionInfo":{"status":"ok","timestamp":1578989605997,"user_tz":360,"elapsed":27134,"user":{"displayName":"Ammar Hoque","photoUrl":"","userId":"02857402399992548760"}},"colab":{"base_uri":"https://localhost:8080/","height":986}},"source":["!pip3 install pyshp\n","!pip3 install geopandas\n","!pip3 install OWSLib\n","!pip3 install zipfile\n","!pip3 install opencv-python\n","!pip3 install imageio\n","!pip3 install tensorflow\n","!pip3 install pickle\n","!pip3 install utils\n","!pip3 install pillow"],"execution_count":34,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pyshp in /usr/local/lib/python3.6/dist-packages (2.1.0)\n","Requirement already satisfied: geopandas in /usr/local/lib/python3.6/dist-packages (0.6.2)\n","Requirement already satisfied: fiona in /usr/local/lib/python3.6/dist-packages (from geopandas) (1.8.13)\n","Requirement already satisfied: shapely in /usr/local/lib/python3.6/dist-packages (from geopandas) (1.6.4.post2)\n","Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from geopandas) (0.25.3)\n","Requirement already satisfied: pyproj in /usr/local/lib/python3.6/dist-packages (from geopandas) (2.4.2.post1)\n","Requirement already satisfied: click<8,>=4.0 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (7.0)\n","Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (0.5.0)\n","Requirement already satisfied: munch in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (2.5.0)\n","Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (1.12.0)\n","Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (1.1.1)\n","Requirement already satisfied: attrs>=17 in /usr/local/lib/python3.6/dist-packages (from fiona->geopandas) (19.3.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->geopandas) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->geopandas) (2.6.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0->geopandas) (1.17.5)\n","Requirement already satisfied: OWSLib in /usr/local/lib/python3.6/dist-packages (0.19.0)\n","Requirement already satisfied: requests>=1.0 in /usr/local/lib/python3.6/dist-packages (from OWSLib) (2.21.0)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from OWSLib) (2018.9)\n","Requirement already satisfied: pyproj in /usr/local/lib/python3.6/dist-packages (from OWSLib) (2.4.2.post1)\n","Requirement already satisfied: python-dateutil>=1.5 in /usr/local/lib/python3.6/dist-packages (from OWSLib) (2.6.1)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=1.0->OWSLib) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=1.0->OWSLib) (2019.11.28)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=1.0->OWSLib) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=1.0->OWSLib) (3.0.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=1.5->OWSLib) (1.12.0)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement zipfile (from versions: none)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for zipfile\u001b[0m\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (4.1.2.30)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python) (1.17.5)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (2.4.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageio) (1.17.5)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio) (6.2.2)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (1.15.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.1)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.33.6)\n","Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.17.5)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.1.8)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow) (42.0.2)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow) (0.16.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow) (3.1.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow) (2.8.0)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for pickle\u001b[0m\n","Requirement already satisfied: utils in /usr/local/lib/python3.6/dist-packages (1.0.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (6.2.2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kwaS7jAL-6T-","colab_type":"text"},"source":["# Dependencies"]},{"cell_type":"code","metadata":{"id":"DS08IS15LrP0","colab_type":"code","outputId":"51a9fcdf-c338-4d66-f0dd-da7dcf308399","executionInfo":{"status":"ok","timestamp":1578990903949,"user_tz":360,"elapsed":2781,"user":{"displayName":"Ammar Hoque","photoUrl":"","userId":"02857402399992548760"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#zip_ref = zipfile.ZipFile('cnn_models.zip')\n","#zip_ref.extractall()\n","\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import scipy.ndimage\n","import imageio\n","\n","import shapefile as shp\n","import json\n","import geopandas as gpd\n","import datetime\n","from owslib.wms import WebMapService\n","\n","import zipfile\n","from collections import defaultdict\n","import cv2\n","from PIL import *\n","from vggnet16 import * #VGGNet is the where the structure of my neural network is written\n","!pwd"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_CGoVAgS7TbW","colab_type":"code","outputId":"b4fa3543-9418-4cba-805f-c3d98d0e938c","executionInfo":{"status":"ok","timestamp":1578818827561,"user_tz":360,"elapsed":201,"user":{"displayName":"Ammar Hoque","photoUrl":"","userId":"02857402399992548760"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","drive.flush_and_unmount"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"TpiHhqG_LrP4","colab_type":"text"},"source":["in the tutorial, the author gets the sattellite image data from the Dutch government open data portal. However, the data has been moved, and I got the rest of the data from the dropbox. I cannot get the satellite image data, so I just used his image tiles, but I know what he is doing. Now we will have to unzip our zipfiles containing the images and shapefiles:"]},{"cell_type":"code","metadata":{"id":"ZTdOY6ZuLrP5","colab_type":"code","colab":{}},"source":["#!pwd\n","#!mkdir shapefile-new\n","#!mkdir shapefile-new/2017_09\n","#!mkdir image_tiles\n","\n","\n","#extracting the shapefiles\n","#zip_ref = zipfile.ZipFile('2017_09.zip')\n","#zip_ref.extractall('shapefile-new')\n","\n","#extracting the image tiles\n","#zip_ref = zipfile.ZipFile('image_tiles_200.zip')\n","#zip_ref.extractall('image_tiles')\n","\n","###here we define the BOUNDING_BOX, or region where we would like to visualize###\n","\n","x_min = 100000 #min x-val (start in our coordinate)\n","y_min = 427000 #min y-val\n","dx, dy = 200, 200 #(the amount of x and y we would want stored from the map in each tile)\n","\n","no_tiles_x = 100 #number of tiles on the grid over the image\n","no_tiles_y = 100 #total tiles we make is image_width = 256\n","image_height = 256\n","image_depth = 3\n","total_no_images = 10000\n"," \n","x_max = x_min + no_tiles_x * dx #max x-val (end in our coordinates)\n","y_max = y_min + no_tiles_y * dy #max y-val \n","\n","BOUNDING_BOX = [x_min, y_min, x_max, y_max] #this is the important global variable we wanted\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"LaWNjJ8ELrP_","colab_type":"text"},"source":["We extracted the zipfiles now. let's read our shapefile and convert its contents into a json so we do not have to repeat the shapefile-reading process each time:"]},{"cell_type":"code","metadata":{"id":"Co8kpQjASge9","colab_type":"code","colab":{}},"source":["filename ='shapefile-new/2017_09/Wegvakken.shp'\n","gdf = gpd.read_file(filename)\n","gdf.to_file(\"gpd_output.json\", driver='GeoJSON')\n","#WOW SAVED SO MUCH MEMORY"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VornWuOnTUSE","colab_type":"code","colab":{}},"source":["gdf.to_file(\"gpd_output.json\", driver='GeoJSON')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"p6mgV-L6LrQM","colab_type":"text"},"source":["# Mapping the layers now. We want to determine which image tiles have roads, and which do not:"]},{"cell_type":"markdown","metadata":{"id":"gkZ4fMoQjfGW","colab_type":"text"},"source":["let's go and define the road types with some variables, whether they are municpality(Gemeente), government(Rijik), etc. We wil also indicate the road color types aswell. I am also writing some keys to get"]},{"cell_type":"code","metadata":{"id":"uWJwBtkaLrQN","colab_type":"code","colab":{}},"source":["####### MAPPING TIME ##########\n","#let's define some important keys first\n","#First we define some variables, and dictionary keys which are going to be used throughout the rest.\n","\n","dict_roadtype = {              #THIS ISN'T NEEDED (but I do like statistics so I'll keep it)\n","    \"G\": 'Gemeente',\n","    \"R\": 'Rijk',\n","    \"P\": 'Provincie',\n","    \"W\": 'Waterschap',\n","    'T': 'Andere wegbeheerder',\n","    '' : 'leeg'\n","}\n","\n","dict_roadtype_to_color = {\n","    \"G\": 'red',\n","    \"R\": 'blue',\n","    \"P\": 'green',\n","    \"W\": 'magenta',\n","    'T': 'yellow',\n","    '' : 'leeg'\n","}\n","\n","FEATURES_KEY = \"features\"\n","PROPERTIES_KEY = \"properties\"\n","GEOMETRY_KEY = \"geometry\"\n","COORDINATES_KEY = \"coordinates\"\n","WEGSOORT_KEY = \"WEGBEHSRT\"\n","\n","MINIMUM_NO_POINTS_PER_TILE = 4\n","POINTS_PER_METER = 0.1\n","\n","INPUT_FOLDER_TILES = 'image_tiles/image_tiles_200/'\n","\n","\n","filename_wegvakken = \"gpd_output.json\"\n","gpd_json = json.load(open(filename_wegvakken))\n","\n","dict_features = gpd_json[FEATURES_KEY] #get all the json data\n","dict_tile_contents = defaultdict(list) #stores tiles to their contents (key:tilename, value:list)\n","dict_roadtype_tiles = defaultdict(set) # stores road-type as its keys and the tiles that have the road type as its value (key:roadttype value:set)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IXw6fnPEY-Cn","colab_type":"code","colab":{}},"source":["print(dict_features)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5ZoeYQkml1Xa","colab_type":"text"},"source":["now that we created  our keys and an our dictionaries, I will create some helper functions from the tutorial that will help us work with this data:"]},{"cell_type":"code","metadata":{"id":"1MNAxaBkXNHK","colab_type":"code","colab":{}},"source":["INPUT_FOLDER_TILES = 'image_tiles/image_tiles_200/'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZfoV0b3XLrQR","colab_type":"code","colab":{}},"source":["def coord_is_in_bb(coord, bb): #check wheter the coordniate is inside the bounding box (bb)\n","    x_min = bb[0]\n","    y_min = bb[1]\n","    x_max = bb[2]\n","    y_max = bb[3]\n","    return ((coord[0] >= x_min) and (coord[0] <= x_max) and (coord[1] >= y_min) and (coord[1] <= y_max)) #return true if x-min < x-coord < x-max and y-min < y-coord < y-max \n"," \n","def get_intermediate_coords(coord1, coord2, num_points):\n","    dx = (coord2[0] - coord1[0])/(num_points + 1) #we add 1 because we want to space between coord2's points and the last intermediate point\n","    dy = (coord2[1] - coord1[1])/(num_points + 1)\n","    return [[coord1[0] + i*dx, coord1[1] + i*dy] for i in range (0, (num_points+1))]\n","\n","\n","def retrieve_roadtype(elem): #get the roadtype value from a particular line geometry\n","    return elem[PROPERTIES_KEY][WEGSOORT_KEY]\n","   \n","def retrieve_coordinates(elem): #get coordinate value\n","    return elem[GEOMETRY_KEY][COORDINATES_KEY]\n","  \n"," \n","def add_to_dict(d1, d2, coordinate, rtype): #get a tile based on the given coordinate\n","    coordinate_ll_x = int((coordinate[0] // dx)*dx) #get lower x of the tile containing the coordinate by doing division with the dx and truncating down then multiplying that by dx\n","    coordinate_ll_y = int((coordinate[1] // dy)*dy) #get lower y of tile by division(trunacte donw) with dy then multiplying that by dy\n","    coordinate_ur_x = int((coordinate[0] // dx)*dx + dx) #get upper x by getting lower x and adding dx\n","    coordinate_ur_y = int((coordinate[1] // dy)*dy + dy) #get upper y by getting lower y then adding dy\n","    tile = \"{}_{}_{}_{}.jpg\".format(coordinate_ll_x, coordinate_ll_y, coordinate_ur_x, coordinate_ur_y)\n","    \n","    rel_coord_x = (coordinate[0] - coordinate_ll_x) / dx #relative coordinates\n","    rel_coord_y = (coordinate[1] - coordinate_ll_y) / dy\n","    value = (rtype, rel_coord_x, rel_coord_y)\n","    d1[tile].append(value)  #for the tile dictionary, we are storing the tile to map with its roadtype with the roadtype's relative coordinates on the tile\n","    \n","    d2[rtype].add(tile) #roadtype will get a new tile added with it This is for statistics only"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wH2AaV4atVrc","colab_type":"text"},"source":["now let's do our data processing"]},{"cell_type":"code","metadata":{"id":"Bd4zx4OSjUoM","colab_type":"code","colab":{}},"source":["for elem in dict_features: #iterate through each geometry\n","  coordinates = retrieve_coordinates(elem) #get the coordinates of the road\n","  road_type = retrieve_roadtype(elem)\n","  coordinates_in_bbox = [coord for coord in coordinates if coord_is_in_bb(coord, BOUNDING_BOX)] #see which coordinates of the linestr geometry are within bounding box\n","  \n","  if len(coordinates_in_bbox) == 1: #only one coordinate is in bounding box\n","    add_to_dict(dict_tile_contents, dict_roadtype_tiles, coord, road_type) #add the coordinate with the roadtype to tiles_dict, add the tile do the roadtype given\n","    \n","  if len(coordinates_in_bbox) > 1:\n","    add_to_dict(dict_tile_contents, dict_roadtype_tiles, coordinates_in_bbox[0], road_type)#add part one\n","    for ii in range(1, len(coordinates_in_bbox)): #loop through each coordinate\n","      prev_coord = coordinates_in_bbox[ii-1]\n","      coord = coordinates_in_bbox[ii]\n","      add_to_dict(dict_tile_contents, dict_roadtype_tiles, coord, road_type)\n","      \n","      dist = np.linalg.norm(np.array(coord)-np.array(prev_coord))\n","      no_intermediates = int(dist/10) #get the number of intermediate points in between two points\n","      \n","      intermediate_coords = get_intermediate_coords(prev_coord, coord, no_intermediates)\n","      \n","      for coord in intermediate_coords:\n","        add_to_dict(dict_tile_contents, dict_roadtype_tiles, coord, road_type)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HVN6sz7eA0Xh","colab_type":"text"},"source":["Time to visualize everything now that we got our data prepared:"]},{"cell_type":"markdown","metadata":{"id":"F-vSH0zH5zkH","colab_type":"text"},"source":["Now we processed the data. **time to visualize it on the map**\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"uyZPp3U55Up2","colab_type":"code","outputId":"5d435df1-2f7d-46a1-dfc5-8b69f0561915","executionInfo":{"status":"ok","timestamp":1558514788006,"user_tz":300,"elapsed":14350,"user":{"displayName":"Ammar Hoque","photoUrl":"","userId":"02857402399992548760"}},"colab":{"base_uri":"https://localhost:8080/","height":901,"output_embedded_package_id":"1i93HqiTRGr3zsBVfKSYK8G7vfD2mwivg"}},"source":["x0 = x_min\n","y0 = y_min\n","fig, axarr = plt.subplots(nrows=11, ncols=11, figsize=(16,16)) #we created 11 rows and 11 columns\n","\n","for ii in range (0,11):\n","  for jj in range(0,11):\n","    ll_x = x0 + ii*dx\n","    ll_y = y0 + jj*dy\n","    ur_x = ll_x + dx\n","    ur_y = ll_y + dy\n","    tilename = '{}_{}_{}_{}.jpg'.format(ll_x, ll_y, ur_x, ur_y)\n","    filename = INPUT_FOLDER_TILES + tilename\n","    tile_contents = dict_tile_contents[tilename] #get the tile contents (array of tuples containing road type & relative coords of the road in that tile)\n","\n","    \n","    ax = axarr[10-jj, ii]\n","    image = plt.imread(filename)\n","    rgb_image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) #convert our map to rgb\n","    ax.imshow(rgb_image)\n","    ax.get_xaxis().set_visible(False)\n","    ax.get_yaxis().set_visible(False)\n","    \n","    for elem in tile_contents:\n","      \n","      color = dict_roadtype_to_color[elem[0]]\n","      x = elem[1]*256 #256 because that's the size of the image (and I need to convert relative distance in the tile to a point on the image)\n","      y = (1-elem[2]) *256\n","      ax.scatter(x,y, c=color, s=10)\n","      \n","plt.subplots_adjust(wspace=0, hspace=0)\n","plt.show()\n","  "],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"v8lrwL8mVtgE","colab_type":"text"},"source":["And here we have a visualization of the roads. the red roads are local roads, pruple roads are roads for water authority management, and blue roads are government authority roads. "]},{"cell_type":"markdown","metadata":{"id":"rOXNnl_RChXX","colab_type":"text"},"source":["# Neural Network to detect the roads"]},{"cell_type":"code","metadata":{"id":"joAIWDFzHdj2","colab_type":"code","colab":{}},"source":["#!mkdir cnn_models\n","!mv /*.py /content/cnn_models"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i-pPPANWCgES","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":63},"outputId":"99c992b0-4045-43b7-a949-d43a1f398ef8","executionInfo":{"status":"ok","timestamp":1578990871283,"user_tz":360,"elapsed":6114,"user":{"displayName":"Ammar Hoque","photoUrl":"","userId":"02857402399992548760"}}},"source":["import tensorflow as tf\n","import pickle #for streaming our data\n","from utils import *\n","from vggnet16 import * #our neural net structure with 16 layers.\n","import os\n","from PIL import Image"],"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"31-hhgdaIwZ3","colab_type":"text"},"source":["## Prepare the dataset"]},{"cell_type":"markdown","metadata":{"id":"DSdHIfVDIhWF","colab_type":"text"},"source":["Now that we have imported our libraries, let's prepare the dataset first. We will start by creating some functions:"]},{"cell_type":"code","metadata":{"id":"UwnIR_5gIfFa","colab_type":"code","colab":{}},"source":["def accuracy(predictions, labels):\n","    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])\n","\n","def onehot_encode_labels(labels):\n","    list_possible_labels = list(np.unique(labels))\n","    encoded_labels = map(lambda x: list_possible_labels.index(x), labels)\n","    return encoded_labels\n","\n","def randomize(dataset, labels1, labels2, labels3):\n","    permutation = np.random.permutation(dataset.shape[0])\n","    randomized_dataset = dataset[permutation, :, :, :]\n","    randomized_labels1 = labels1[permutation]\n","    randomized_labels2 = labels2[permutation]\n","    randomized_labels3 = labels3[permutation]\n","    return randomized_dataset, randomized_labels1, randomized_labels2, randomized_labels3\n","\n","def one_hot_encode(np_array, num_unique_labels): #for randomizing our dataset\n","    return (np.arange(num_unique_labels) == np_array[:,None]).astype(np.float32)\n","\n","def reformat_data(dataset, labels1, labels2, labels3):\n","    dataset, labels1, labels2, labels3 = randomize(dataset, labels1, labels2, labels3)\n","    num_unique_labels1 = len(np.unique(labels1))\n","    num_unique_labels2 = len(np.unique(labels2))\n","    labels1 = one_hot_encode(labels1, num_unique_labels1)\n","    labels2 = one_hot_encode(labels2, num_unique_labels2)\n","    return dataset, labels1, labels2, labels3"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1DH33iuXJEfG","colab_type":"text"},"source":["time to prepare our dataset. We set this picture at 256x256, as this is standard. We also have 10,000 images to work with."]},{"cell_type":"code","metadata":{"id":"43eTS0q9JFat","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":231},"outputId":"2b80fb67-f6cb-43fc-e269-a1194c522262","executionInfo":{"status":"error","timestamp":1578990893157,"user_tz":360,"elapsed":298,"user":{"displayName":"Ammar Hoque","photoUrl":"","userId":"02857402399992548760"}}},"source":["image_width = 256\n","image_height = 256\n","image_depth = 3\n","total_no_images = 10000\n","\n","image_files = os.listdir(INPUT_FOLDER_TILES) #getting the directory of files\n","\n","dataset = np.ndarray(shape=(total_no_images, image_width, image_height, image_depth), dtype=np.float32)\n","labels_roadtype = []\n","labels_roadpresence = np.ndarray(total_no_images, dtype=np.float32)\n","labels_filename = []\n"],"execution_count":5,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-807a965ac112>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mimage_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINPUT_FOLDER_TILES\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#getting the directory of files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_no_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mlabels_roadtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlabels_roadpresence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_no_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"mZOZBe6-J9lZ","colab_type":"text"},"source":["From our tile contents dictionary, we will get the attributes of each image tile and turn it into an array of numerical data"]},{"cell_type":"code","metadata":{"id":"bOd0-143KW81","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":187},"outputId":"6d2f3dfa-8217-4887-97c0-a15bff359763","executionInfo":{"status":"ok","timestamp":1578990660544,"user_tz":360,"elapsed":40429,"user":{"displayName":"Ammar Hoque","photoUrl":"","userId":"02857402399992548760"}}},"source":["for counter, image in enumerate(image_files):\n","    filename = INPUT_FOLDER_TILES + image\n","    labels_filename.append(image)\n","    if image in list(dict_tile_contents.keys()): # go through the image tiles and get the attributes\n","        tile_contents = dict_tile_contents[image]\n","        roadtypes = sorted(list(set([elem[0] for elem in tile_contents])))\n","        roadtype = \"_\".join(roadtypes)\n","        labels_roadpresence[counter] = 1\n","    else:\n","        roadtype = ''\n","        labels_roadpresence[counter] = 0\n","    labels_roadtype.append(roadtype)\n","\n","    img = Image.open(filename) \n","    image_data = np.array(img).astype(np.float32)\n","    dataset[counter, :, :] = image_data\n","    if counter % 1000 == 0:\n","        print(\"{} images have been loaded.\".format(counter))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["0 images have been loaded.\n","1000 images have been loaded.\n","2000 images have been loaded.\n","3000 images have been loaded.\n","4000 images have been loaded.\n","5000 images have been loaded.\n","6000 images have been loaded.\n","7000 images have been loaded.\n","8000 images have been loaded.\n","9000 images have been loaded.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QEpJoEanMkB5","colab_type":"text"},"source":["Now that we obtain the attributes, we will resample the data in order to reduce the test-error rate in our model:"]},{"cell_type":"code","metadata":{"id":"MoTaY1OpNIuE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"2f86bebf-7970-4680-951d-7e64e39a418a","executionInfo":{"status":"ok","timestamp":1578990691545,"user_tz":360,"elapsed":3361,"user":{"displayName":"Ammar Hoque","photoUrl":"","userId":"02857402399992548760"}}},"source":["labels_filename = np.array(labels_filename) #store lables\n","labels_roadtype_ohe = np.array(list(onehot_encode_labels(labels_roadtype)))\n","print(\"Randomizing dataset...\")\n","dataset, labels_roadpresence, labels_roadtype_ohe, labels_filename = reformat_data(dataset, labels_roadpresence, labels_roadtype_ohe, labels_filename)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Randomizing dataset...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"km4Rpo3YNO-H","colab_type":"text"},"source":["let's pickle our dataset so the computer can read the serialized data faster (as we have converted the data into a serial format that the computer can convert to instructions quicker):"]},{"cell_type":"code","metadata":{"id":"e-Qm7-WONfvs","colab_type":"code","colab":{}},"source":["#!mkdir data\n","\n","start_train_dataset = 0\n","start_valid_dataset = 1200\n","start_test_dataset = 1600\n","total_no_images = 10000 \n","\n","output_pickle_file = 'data/sattelite_dataset.pickle'\n","\n","f = open(output_pickle_file, 'wb')\n","save = { # here is our object to train the CNN\n","'train_dataset': dataset[start_train_dataset:start_valid_dataset,:,:,:],\n","'train_labels_roadtype': labels_roadtype[start_train_dataset:start_valid_dataset],\n","'train_labels_roadpresence': labels_roadpresence[start_train_dataset:start_valid_dataset],\n","'train_labels_filename': labels_filename[start_train_dataset:start_valid_dataset],\n","'valid_dataset': dataset[start_valid_dataset:start_test_dataset,:,:,:],\n","'valid_labels_roadtype': labels_roadtype[start_valid_dataset:start_test_dataset],\n","'valid_labels_roadpresence': labels_roadpresence[start_valid_dataset:start_test_dataset],\n","'valid_labels_filename': labels_filename[start_valid_dataset:start_test_dataset],\n","'test_dataset': dataset[start_test_dataset:total_no_images,:,:,:],\n","'test_labels_roadtype': labels_roadtype[start_test_dataset:total_no_images],\n","'test_labels_roadpresence': labels_roadpresence[start_test_dataset:total_no_images],\n","'test_labels_filename': labels_filename[start_test_dataset:total_no_images]\n","}\n","pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n","f.close()\n","\n","print(\"\\nsaved dataset to {}\".format(output_pickle_file))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vzABdhyUNkD_","colab_type":"text"},"source":["## Neural Network time : Constructing the graph"]},{"cell_type":"code","metadata":{"id":"AaCq4rY-Njgl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":231},"outputId":"6af212f8-cd4f-4f5f-c2bd-ff741649e0fc","executionInfo":{"status":"error","timestamp":1578991012029,"user_tz":360,"elapsed":300,"user":{"displayName":"Ammar Hoque","photoUrl":"","userId":"02857402399992548760"}}},"source":["### SERIALIZE THE DATA ###\n","pickle_file = 'data/sattelite_dataset.pickle'\n","f = open(pickle_file, 'rb')\n","save = pickle.load(f)\n","\n","train_dataset = save['train_dataset'].astype(dtype = np.float32) #cast our data into floating points to get accurate values\n","train_labels = save['train_labels_roadpresence'].astype(dtype = np.float32)\n","valid_dataset = save['valid_dataset'].astype(dtype = np.float32)\n","valid_labels = save['valid_labels_roadpresence'].astype(dtype = np.float32)\n","test_dataset = save['test_dataset'].astype(dtype = np.float32)\n","test_labels = save['test_labels_roadpresence'].astype(dtype = np.float32)\n","f.close()"],"execution_count":7,"outputs":[{"output_type":"error","ename":"EOFError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-9199b72c4c90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpickle_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/sattelite_dataset.pickle'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_dataset'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#cast our data into floating points to get accurate values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mEOFError\u001b[0m: Ran out of input"]}]},{"cell_type":"markdown","metadata":{"id":"oGTcz2YfO8qa","colab_type":"text"},"source":["here we set our values for parameters in the network"]},{"cell_type":"code","metadata":{"id":"rdP6UtdeO8I4","colab_type":"code","colab":{}},"source":["num_labels = len(np.unique(train_labels))\n","image_width = 256\n","image_height = 256\n","image_depth = 3\n","num_steps = 501\n","display_step = 10\n","learning_rate = 0.0001\n","batch_size = 16\n","lambda_loss_amount = 0.0015"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ElXhHOk4QPHq","colab_type":"text"},"source":["Let's construct the graph:"]},{"cell_type":"code","metadata":{"id":"um3a0Eu2P9aL","colab_type":"code","colab":{}},"source":["train_accuracies, test_accuracies, valid_accuracies = [], [], []\n"," \n","print(\"STARTING WITH SATTELITE\")\n","graph = tf.Graph()\n","with graph.as_default():\n","    #1) First we put the input data in a tensorflow friendly form. \n","    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_width, image_height, image_depth))\n","    tf_train_labels = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n","    tf_test_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_width, image_height, image_depth))\n","    tf_test_labels = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n","    tf_valid_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_width, image_height, image_depth))\n","    tf_valid_labels = tf.placeholder(tf.float32, shape = (batch_size, num_labels))\n"," \n","    #2) Then, the weight matrices and bias vectors are initialized\n","    variables = variables_vggnet16()\n"," \n","    #3. The model used to calculate the logits (predicted labels)\n","    model = model_vggnet16\n","    \n","    logits = model(tf_train_dataset, variables)\n"," \n","    #4. then we compute the softmax cross entropy between the logits and the (actual) labels\n","    l2 = lambda_loss_amount * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n","    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + l2\n"," \n","    #learning_rate = tf.train.exponential_decay(0.05, global_step, 1000, 0.85, staircase=True)\n","    #5. The optimizer is used to calculate the gradients of the loss function \n","    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n"," \n","    # Predictions for the training, validation, and test data.\n","    train_prediction = tf.nn.softmax(logits)\n","    test_prediction = tf.nn.softmax(model(tf_test_dataset, variables))\n","    valid_prediction = tf.nn.softmax(model(tf_valid_dataset, variables))\n"," \n"," \n","with tf.Session(graph=graph) as session:\n","    test_counter = 0\n","    tf.global_variables_initializer().run()\n","    print('Initialized with learning_rate', learning_rate, \" model \", ii)\n","    for step in range(num_steps):\n","        #Since we are using stochastic gradient descent, we are selecting  small batches from the training dataset,\n","        #and training the convolutional neural network each time with a batch. \n","        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n","        batch_data = train_dataset[offset:(offset + batch_size), :,  :]\n","        batch_labels = train_labels[offset:(offset + batch_size), :]\n","        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n","        \n","        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n","        train_accuracy = accuracy(predictions, batch_labels)\n","        train_accuracies.append(train_accuracy)\n"," \n","        if step % display_step == 0:\n","            offset2 = (test_counter * batch_size) % (test_labels.shape[0] - batch_size)\n","            test_dataset_batch = test_dataset[offset2:(offset2 + batch_size), :, :]\n","            test_labels_batch = test_labels[offset2:(offset2 + batch_size), :]\n","            feed_dict2 = {tf_test_dataset : test_dataset_batch, tf_test_labels : test_labels_batch}\n","            \n","            test_prediction_ = session.run(test_prediction, feed_dict=feed_dict2)\n","            test_accuracy = accuracy(test_prediction_, test_labels_batch)\n","            test_accuracies.append(test_accuracy)\n"," \n","            valid_dataset_batch = valid_dataset[offset2:(offset2 + batch_size), :, :]\n","            valid_labels_batch = valid_labels[offset2:(offset2 + batch_size), :]\n","            feed_dict3 = {tf_valid_dataset : valid_dataset_batch, tf_valid_labels : valid_labels_batch}\n","            \n","            valid_prediction_ = session.run(valid_prediction, feed_dict=feed_dict3)\n","            valid_accuracy = accuracy(valid_prediction_, valid_labels_batch)\n","            valid_accuracies.append(valid_accuracy)\n"," \n","            message = \"step {:04d} : loss is {:06.2f}, accuracy on training set {:02.2f} %, accuracy on test set {:02.2f} accuracy on valid set {:02.2f} %\".format(step, l, train_accuracy, test_accuracy, valid_accuracy)\n","            print(message)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O5kSptwiQh66","colab_type":"text"},"source":["## Neural Network time: visualizing our results."]},{"cell_type":"markdown","metadata":{"id":"R7HoP9WLQwpy","colab_type":"text"},"source":["First, I will define a function to get the average number of points per image tile: BIG QUESTION ON THIS FUNCTION"]},{"cell_type":"code","metadata":{"id":"3m-4xIWqRReM","colab_type":"code","colab":{}},"source":["##WTF DO I DO?##\n","def average_points(points): \n","    averaged_points = []\n","    for ii in range(10,len(points),10):\n","        subsection  = points[ii-10:ii]\n","        average = np.nanmean(subsection)\n","        averaged_points.append(average)\n","    return averaged_points"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qcYWLxwpRjhn","colab_type":"text"},"source":["Time to plot. We will be visualizing the Training accuracy, the test accuracy, and cross-validation accuracy. Here is the plot of the three accuracies:"]},{"cell_type":"code","metadata":{"id":"8YnzAW4IRlmG","colab_type":"code","colab":{}},"source":["num_steps = 501\n","ylimit = [0,100]\n","labels = ['Train accuracy', 'Test accuracy', 'Validation accuracy']\n","ylabel = \"Accuracy [%]\"\n","xlabel = \"Number of Iterations\"\n","title = \"Accuracy of road detection in Aerial Images\"\n","colors = ['r', 'g', 'b']\n","\n","list_accuracies = [train_accuracies, test_accuracies, valid_accuracies]\n","\n","fig, ax = plt.subplots(figsize=(12,8))\n","ax.set_ylim(ylimit)\n","ax.set_ylabel(ylabel, fontsize=16)\n","ax.set_xlabel(xlabel, fontsize=16)\n","ax.set_title(title, fontsize=20)\n","\n","\n","for ii, accuracies in enumerate(list_accuracies):\n","    color = colors[ii]\n","    label = labels[ii]\n","    if ii > 0:\n","        y_values = accuracies\n","        x_values = range(0,num_steps, 10)\n","        ax.plot(x_values, y_values, '.-{}'.format(color), label = label)\n","    else:\n","        y_values_ = accuracies\n","        y_values = average_points(y_values_, 5)\n","        x_values = range(1,len(y_values_),5)\n","        ax.plot(x_values, y_values, '.{}'.format(color), label = label)\n","ax.legend(loc='lower right')\n","plt.show()"],"execution_count":0,"outputs":[]}]}